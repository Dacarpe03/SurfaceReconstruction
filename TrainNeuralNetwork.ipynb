{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3f9bef8",
   "metadata": {},
   "source": [
    "# 0. Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a2f8ed",
   "metadata": {},
   "source": [
    "The libraries used to create the data are in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0741bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-20 12:52:49.333186: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-20 12:52:49.370515: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-20 12:52:49.371411: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-20 12:52:49.903709: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-09-20 12:52:51.547300: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-20 12:52:51.547869: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_visible_devices(physical_devices[0], 'GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2879df",
   "metadata": {},
   "source": [
    "# 1. Define constants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce63383a",
   "metadata": {},
   "source": [
    "This section defines constants to be used in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ebca1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path constants\n",
    "DATA_FOLDER_PATH = \"./Data\"\n",
    "FEATURES_FILENAME = \"surfaces.npy\"\n",
    "LABELS_FILENAME = \"coefficients.npy\"\n",
    "\n",
    "FEATURES_FILE_PATH = f\"{DATA_FOLDER_PATH}/{FEATURES_FILENAME}\"\n",
    "LABELS_FILE_PATH = f\"{DATA_FOLDER_PATH}/{LABELS_FILENAME}\"\n",
    "\n",
    "# Column names\n",
    "SURFACE_POINTS = \"surface_points\"\n",
    "ZERNIKE_COEFFICIENTS = \"zernike_coefficients\"\n",
    "\n",
    "# Neural network architecture size\n",
    "INPUT_SIZE = 60\n",
    "OUTPUT_SIZE = 10\n",
    "\n",
    "# Neural network training parameters\n",
    "N_EPOCHS = 100\n",
    "BATCH_SIZE = 4096\n",
    "LEARNING_RATE = 0.01\n",
    "ACTIVATION = 'relu'\n",
    "N_HIDDEN = [64, 32, 16]\n",
    "REGULARIZER = keras.regularizers.L1L2(l1=0.001,l2=0.1)\n",
    "INITIALIZER = keras.initializers.he_normal(seed=None)\n",
    "LOSS = tf.keras.losses.MeanSquaredError()\n",
    "OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, beta_1=0.9, beta_2=0.999)\n",
    "METRICS = tf.keras.metrics.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60444c6d",
   "metadata": {},
   "source": [
    "# 2. Compile functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c512511c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_and_labels_from_df(df):\n",
    "    \"\"\"\n",
    "    Converts the columns of a surface dataframe into a list of features and labels in the form of list of numpy arrays\n",
    "    Input:\n",
    "        df (pd.DataFrame): The surface dataframe with columns 'surface_points'(features) and 'zernike_coefficients'(labels)\n",
    "    \n",
    "    Output:\n",
    "        features (np.array): A numpy array with the z values of the surface at sampled points\n",
    "        labels (np.array): A numpy array with the Zernike coefficients of the surface\n",
    "    \"\"\"\n",
    "    features = convert_column_into_numpy_array_list(df, SURFACE_POINTS)\n",
    "    labels = convert_column_into_numpy_array_list(df, ZERNIKE_COEFFICIENTS)\n",
    "    return features, labels\n",
    "    \n",
    "\n",
    "def convert_column_into_numpy_array_list(df, column_name):\n",
    "    \"\"\"\n",
    "    Converts the specified column of the dataframe into a list of numpy arrays\n",
    "    Input:\n",
    "        df (pd.DataFrame): The surface dataframe with columns 'surface_points' and 'zernike_coefficients'\n",
    "        column_name (string): The name of the column to convert to list of numpy arrays\n",
    "        \n",
    "    Returns:\n",
    "        numpy_list (tensor): The list of numpy arrays converted from the column\n",
    "    \"\"\"\n",
    "    numpy_series = df[column_name].apply(lambda x: np.fromstring(x[1: -1], dtype=float, sep=' ')).to_list()\n",
    "    numpy_list = tf.data.Dataset.from_tensor_slices(numpy_series)\n",
    "    return numpy_series\n",
    "\n",
    "\n",
    "def create_architecture():\n",
    "    model = keras.Sequential(name=\"SurfaceReconstructor\")\n",
    "    model.add(keras.layers.InputLayer(input_shape=(INPUT_SIZE,),\n",
    "                                      batch_size=None))\n",
    "    \n",
    "    for neurons in N_HIDDEN:\n",
    "        model.add(keras.layers.Dense(neurons,\n",
    "                                     kernel_regularizer=REGULARIZER,\n",
    "                                     kernel_initializer=keras.initializers.HeNormal(seed=None),\n",
    "                                     use_bias=False\n",
    "                                     ))\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "        model.add(keras.layers.Activation(ACTIVATION))\n",
    "        \n",
    "    model.add(keras.layers.Dense(OUTPUT_SIZE,\n",
    "                                 activation=\"softmax\"\n",
    "                                ))\n",
    "    \n",
    "    model_name = \"final\"\n",
    "    return model, model_name\n",
    "\n",
    "\n",
    "def compile_model(my_model):\n",
    "    my_model.compile(loss=LOSS,\n",
    "                     optimizer=OPTIMIZER,\n",
    "                     metrics=[METRICS])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bd56de",
   "metadata": {},
   "source": [
    "# 2. Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc99edb1",
   "metadata": {},
   "source": [
    "First read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e369026",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.load(FEATURES_FILE_PATH, allow_pickle=True)\n",
    "labels = np.load(LABELS_FILE_PATH, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e321dc",
   "metadata": {},
   "source": [
    "Split the data:\n",
    "- **80%** training\n",
    "- **10%** validation/dev test\n",
    "- **10%** final test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "356b0b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of rows for each split\n",
    "total_rows = len(features)\n",
    "train_size = int(0.8 * total_rows)\n",
    "val_size = int(0.1 * total_rows)\n",
    "\n",
    "# Split the DataFrame into training (80%), validation (10%), and test (10%)\n",
    "train_features = features[:train_size]\n",
    "train_labels = labels[:train_size]\n",
    "\n",
    "val_features = features[train_size:train_size + val_size]\n",
    "val_labels = labels[train_size:train_size + val_size]\n",
    "\n",
    "test_features = features[train_size + val_size:]\n",
    "test_labels = labels[train_size + val_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adafc4a3",
   "metadata": {},
   "source": [
    "# 3. Train neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6647de92",
   "metadata": {},
   "source": [
    "Create the neural network architecture and compile it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b68874cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_reconstruction_model, model_name = create_architecture()\n",
    "compile_model(surface_reconstruction_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3293f9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "12/12 [==============================] - 1s 21ms/step - loss: 48.2524 - mean_squared_error: 33.1116 - val_loss: 40.6902 - val_mean_squared_error: 33.0567\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 37.1431 - mean_squared_error: 32.5140 - val_loss: 35.3018 - val_mean_squared_error: 33.1832\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 33.4479 - mean_squared_error: 32.1921 - val_loss: 33.8551 - val_mean_squared_error: 33.2820\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 32.4405 - mean_squared_error: 32.0814 - val_loss: 33.5231 - val_mean_squared_error: 33.3317\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 32.2089 - mean_squared_error: 32.0690 - val_loss: 33.4381 - val_mean_squared_error: 33.3376\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 32.1447 - mean_squared_error: 32.0615 - val_loss: 33.4082 - val_mean_squared_error: 33.3351\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 32.1317 - mean_squared_error: 32.0591 - val_loss: 33.3804 - val_mean_squared_error: 33.3152\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 32.1101 - mean_squared_error: 32.0558 - val_loss: 33.3759 - val_mean_squared_error: 33.3171\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 32.1362 - mean_squared_error: 32.0618 - val_loss: 33.3167 - val_mean_squared_error: 33.2434\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 32.1012 - mean_squared_error: 32.0433 - val_loss: 33.3590 - val_mean_squared_error: 33.3151\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 32.1096 - mean_squared_error: 32.0583 - val_loss: 33.3084 - val_mean_squared_error: 33.2437\n",
      "Epoch 12/100\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 32.2378 - mean_squared_error: 32.1731"
     ]
    }
   ],
   "source": [
    "history = surface_reconstruction_model.fit(train_features,\n",
    "                                           train_labels,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           epochs=N_EPOCHS,\n",
    "                                           validation_data= (val_features, val_labels),\n",
    "                                           verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a62b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def string_to_array(string):\n",
    "    return np.array([float(x) for x in string.split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f02e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some example data\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 10)  # 100 samples, 10 features\n",
    "y = np.random.randint(0, 2, size=100)  # Binary classification labels (0 or 1)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(X) * split_ratio)\n",
    "\n",
    "X_train, X_val = X[:split_index], X[split_index:]\n",
    "y_train, y_val = y[:split_index], y[split_index:]\n",
    "\n",
    "# Define the neural network model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation='relu', input_shape=(10,)),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')  # Binary classification output\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate the model on the validation data\n",
    "val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247258c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=pd.DataFrame(history.history)\n",
    "results.plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.xlabel (\"Epochs\")\n",
    "plt.ylabel (\"Mean Squared Error\")\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dffc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f384f810",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iac-surface-env",
   "language": "python",
   "name": "iac-surface-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
