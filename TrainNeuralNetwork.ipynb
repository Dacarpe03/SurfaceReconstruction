{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3f9bef8",
   "metadata": {},
   "source": [
    "# 0. Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a2f8ed",
   "metadata": {},
   "source": [
    "The libraries used to create the data are in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0741bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 13:09:59.655203: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2879df",
   "metadata": {},
   "source": [
    "# 1. Define constants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce63383a",
   "metadata": {},
   "source": [
    "This section defines constants to be used in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebca1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path constants\n",
    "DATA_FOLDER_PATH = \"./Data\"\n",
    "DATA_FILENAME = \"surfaces.csv\"\n",
    "DATA_FILE_PATH = f\"{DATA_FOLDER_PATH}/{DATA_FILENAME}\"\n",
    "\n",
    "# Column names\n",
    "SURFACE_POINTS = \"surface_points\"\n",
    "ZERNIKE_COEFFICIENTS = \"zernike_coefficients\"\n",
    "\n",
    "# Neural network architecture size\n",
    "INPUT_SIZE = 60\n",
    "OUTPUT_SIZE = 10\n",
    "\n",
    "# Neural network training parameters\n",
    "N_EPOCHS = 5000\n",
    "BATCH_SIZE = 4096\n",
    "LEARNING_RATE = 0.01\n",
    "ACTIVATION = 'relu'\n",
    "N_HIDDEN = [64, 32, 16]\n",
    "REGULARIZER = keras.regularizers.L1L2(l1=0.001,l2=0.1)\n",
    "INITIALIZER = keras.initializers.he_normal(seed=None)\n",
    "LOSS = tf.keras.losses.MeanSquaredError()\n",
    "OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, beta_1=0.9, beta_2=0.999)\n",
    "METRICS = [tf.keras.metrics.MeanSquaredError()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60444c6d",
   "metadata": {},
   "source": [
    "# 2. Compile functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c512511c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_and_labels_from_df(df):\n",
    "    \"\"\"\n",
    "    Converts the columns of a surface dataframe into a list of features and labels in the form of list of numpy arrays\n",
    "    Input:\n",
    "        df (pd.DataFrame): The surface dataframe with columns 'surface_points'(features) and 'zernike_coefficients'(labels)\n",
    "    \n",
    "    Output:\n",
    "        features (np.array): A numpy array with the z values of the surface at sampled points\n",
    "        labels (np.array): A numpy array with the Zernike coefficients of the surface\n",
    "    \"\"\"\n",
    "    features = convert_column_into_numpy_array_list(df, SURFACE_POINTS)\n",
    "    labels = convert_column_into_numpy_array_list(df, ZERNIKE_COEFFICIENTS)\n",
    "    return features, labels\n",
    "    \n",
    "\n",
    "def convert_column_into_numpy_array_list(df, column_name):\n",
    "    \"\"\"\n",
    "    Converts the specified column of the dataframe into a list of numpy arrays\n",
    "    Input:\n",
    "        df (pd.DataFrame): The surface dataframe with columns 'surface_points' and 'zernike_coefficients'\n",
    "        column_name (string): The name of the column to convert to list of numpy arrays\n",
    "        \n",
    "    Returns:\n",
    "        numpy_list (list): The list of numpy arrays converted from the column\n",
    "    \"\"\"\n",
    "    numpy_list = df[column_name].apply(lambda x: np.fromstring(x[1: -1], dtype=float, sep=' '))\n",
    "    return numpy_list.values\n",
    "\n",
    "\n",
    "def create_architecture():\n",
    "    model = keras.Sequential(name=\"SurfaceReconstructor\")\n",
    "    model.add(keras.layers.InputLayer(input_shape=(INPUT_SIZE,),\n",
    "                                      batch_size=None))\n",
    "    \n",
    "    for neurons in N_HIDDEN:\n",
    "        model.add(keras.layers.Dense(neurons,\n",
    "                                     kernel_regularizer=REGULARIZER,\n",
    "                                     kernel_initializer=keras.initializers.HeNormal(seed=None),\n",
    "                                     use_bias=False\n",
    "                                     ))\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "        model.add(keras.layers.Activation(ACTIVATION))\n",
    "        \n",
    "    model.add(keras.layers.Dense(OUTPUT_SIZE,\n",
    "                                 activation=\"softmax\"\n",
    "                                ))\n",
    "    \n",
    "    model_name = \"final\"\n",
    "    return model, model_name\n",
    "\n",
    "\n",
    "def compile_model(model):\n",
    "    my_model.compile(loss=LOSS,\n",
    "                     optimizer=OPTIMIZER,\n",
    "                     metrics=METRICS)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bd56de",
   "metadata": {},
   "source": [
    "# 2. Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc99edb1",
   "metadata": {},
   "source": [
    "First read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e369026",
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_df = pd.read_csv(DATA_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e321dc",
   "metadata": {},
   "source": [
    "Split the data:\n",
    "- **80%** training\n",
    "- **10%** validation/dev test\n",
    "- **10%** final test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356b0b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of rows for each split\n",
    "total_rows = len(surface_df)\n",
    "train_size = int(0.8 * total_rows)\n",
    "val_size = int(0.1 * total_rows)\n",
    "\n",
    "# Split the DataFrame into training (80%), validation (10%), and test (10%)\n",
    "train_df = surface_df.iloc[:train_size]\n",
    "val_df = surface_df.iloc[train_size:train_size + val_size]\n",
    "test_df = surface_df.iloc[train_size + val_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adafc4a3",
   "metadata": {},
   "source": [
    "# 3. Train neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6a4a1d",
   "metadata": {},
   "source": [
    "Obtain features and labels from train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1108573e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, labels_train = get_features_and_labels_from_df(train_df)\n",
    "features_val, labels_val = get_features_and_labels_from_df(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6647de92",
   "metadata": {},
   "source": [
    "Create the neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68874cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_reconstruction_model = create_architecture()\n",
    "compile_model(surface_reconstruction_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3293f9ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iac-env",
   "language": "python",
   "name": "iac-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
